{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdwSIjgUhbDd6hQqyJEc/F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suryavamshi866/speech-emotion-recognition/blob/main/Untitled67.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install os numpy librosa keras soundfile"
      ],
      "metadata": {
        "id": "JFK5Ly2yHYjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDVXrY5kHWyL",
        "outputId": "3f34efaa-d7bf-4f46-8658-1e60baf08a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model loaded successfully!\n",
            "\n",
            "--- Prediction Result ---\n",
            "File: recorded_audio.wav\n",
            "Confidence Scores: {'angry': np.float32(0.027), 'disgust': np.float32(0.061), 'fear': np.float32(0.826), 'happy': np.float32(0.002), 'neutral': np.float32(0.055), 'ps': np.float32(0.027), 'sad': np.float32(0.002)}\n",
            "ðŸŽ¯ Predicted Emotion: FEAR\n",
            "-------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from keras.models import load_model\n",
        "# Use soundfile for reading audio to explicitly avoid common audioread/backend issues\n",
        "import soundfile as sf\n",
        "\n",
        "# --- Environment Setup (Critical for avoiding system conflicts) ---\n",
        "# Force TensorFlow to run on CPU if you don't need a GPU (good for debugging)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "# Hide TensorFlow logs (0=show all, 3=hide all warnings/errors)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "# --- Global Constants ---\n",
        "MODEL_PATH = \"speech_emotion_model.h5\"\n",
        "AUDIO_PATH = \"recorded_audio.wav\"\n",
        "EMOTION_LABELS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'ps', 'sad']\n",
        "N_MFCC = 40\n",
        "\n",
        "# --- Model Loading ---\n",
        "try:\n",
        "    # Ensure the model is loaded safely\n",
        "    model = load_model(MODEL_PATH, compile=False)\n",
        "    print(\"âœ… Model loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ Error: Model file not found at {MODEL_PATH}\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading model: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Feature Extraction Function ---\n",
        "def extract_mfcc(filename: str):\n",
        "    \"\"\"\n",
        "    Extracts MFCC features from an audio file using soundfile/librosa.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use soundfile to load the audio data, which is more stable than librosa.load's default backend\n",
        "        y, sr = sf.read(filename, dtype='float32')\n",
        "\n",
        "        # If the audio is stereo, convert it to mono\n",
        "        if len(y.shape) > 1:\n",
        "            y = librosa.to_mono(y.T)\n",
        "\n",
        "        # Resample y to a standard rate if needed, though librosa.feature.mfcc handles this\n",
        "        # with its 'sr' argument. Using sr=None during sf.read reads the original rate.\n",
        "\n",
        "        # Trim the signal to the first 5 seconds after 0.5s offset, similar to the original code logic\n",
        "        start_sample = int(0.5 * sr)\n",
        "        end_sample = start_sample + int(5 * sr)\n",
        "        y = y[start_sample:end_sample]\n",
        "\n",
        "        # Extract MFCCs\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)\n",
        "        # Calculate the mean across time (axis=1 in the new librosa versions)\n",
        "        mfcc_mean = np.mean(mfcc.T, axis=0)\n",
        "\n",
        "        return mfcc_mean\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ Error: Audio file not found at {filename}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during feature extraction: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Prediction Function ---\n",
        "def predict_emotion(audio_path: str = AUDIO_PATH):\n",
        "    \"\"\"\n",
        "    Loads audio, extracts features, and predicts the emotion.\n",
        "    \"\"\"\n",
        "    mfcc = extract_mfcc(audio_path)\n",
        "\n",
        "    if mfcc is None:\n",
        "        return\n",
        "\n",
        "    # Keras expects shape (batch_size, time_steps, features) or (batch_size, time_steps, features, 1)\n",
        "    # Since we are using mean MFCC, we need (1, features, 1)\n",
        "    # The first dim is batch_size (1), the third dim is the channel (1)\n",
        "    mfcc = np.expand_dims(mfcc, axis=0) # Shape (1, 40)\n",
        "    mfcc = np.expand_dims(mfcc, axis=-1) # Shape (1, 40, 1)\n",
        "\n",
        "    try:\n",
        "        prediction = model.predict(mfcc, verbose=0)\n",
        "        emotion_index = np.argmax(prediction)\n",
        "        emotion = EMOTION_LABELS[emotion_index]\n",
        "\n",
        "        print(f\"\\n--- Prediction Result ---\")\n",
        "        print(f\"File: {audio_path}\")\n",
        "        print(f\"Confidence Scores: {dict(zip(EMOTION_LABELS, prediction[0].round(3)))}\")\n",
        "        print(f\"ðŸŽ¯ Predicted Emotion: {emotion.upper()}\")\n",
        "        print(\"-------------------------\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during model prediction: {e}\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    predict_emotion()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install portaudio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJLlmakaHyRF",
        "outputId": "a4c74e43-a9cd-425b-d98c-058ac2547436"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement portaudio (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for portaudio\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyaudio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uinGnGRiH_co",
        "outputId": "9888f6e4-a68e-4001-cbfb-57ed98fed5d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyaudio\n",
            "  Downloading PyAudio-0.2.14.tar.gz (47 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/47.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyaudio\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31mÃ—\u001b[0m \u001b[32mBuilding wheel for pyaudio \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for pyaudio\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build pyaudio\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pyaudio)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import pyaudio\n",
        "import wave\n",
        "from keras.models import load_model\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model(\"speech_emotion_model.h5\")\n",
        "print(\"âœ… Model loaded successfully!\")\n",
        "\n",
        "# Emotion labels (update if your model uses different ones)\n",
        "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'ps', 'sad']\n",
        "\n",
        "# Audio recording configuration\n",
        "FORMAT = pyaudio.paInt16  # 16-bit resolution\n",
        "CHANNELS = 1              # Mono audio\n",
        "RATE = 44100              # Sampling rate\n",
        "CHUNK = 1024              # Buffer size\n",
        "RECORD_SECONDS = 5        # Recording duration\n",
        "WAVE_OUTPUT_FILENAME = \"recorded_audio.wav\"\n",
        "\n",
        "\n",
        "def record_audio():\n",
        "    \"\"\"Record 5 seconds of audio from the microphone and save it as a WAV file.\"\"\"\n",
        "    audio = pyaudio.PyAudio()\n",
        "    print(\"ðŸŽ¤ Recording for 5 seconds... Speak now!\")\n",
        "\n",
        "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
        "                        rate=RATE, input=True,\n",
        "                        frames_per_buffer=CHUNK)\n",
        "    frames = []\n",
        "\n",
        "    for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
        "        data = stream.read(CHUNK)\n",
        "        frames.append(data)\n",
        "\n",
        "    print(\"âœ… Recording complete!\")\n",
        "\n",
        "    stream.stop_stream()\n",
        "    stream.close()\n",
        "    audio.terminate()\n",
        "\n",
        "    # Save the recorded audio\n",
        "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
        "    wf.setnchannels(CHANNELS)\n",
        "    wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
        "    wf.setframerate(RATE)\n",
        "    wf.writeframes(b''.join(frames))\n",
        "    wf.close()\n",
        "\n",
        "    return WAVE_OUTPUT_FILENAME\n",
        "\n",
        "\n",
        "def extract_mfcc(filename):\n",
        "    \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
        "    y, sr = librosa.load(filename, duration=5, offset=0.5)\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
        "    return mfcc\n",
        "\n",
        "\n",
        "def predict_emotion():\n",
        "    \"\"\"Record voice, extract MFCC, and predict emotion.\"\"\"\n",
        "    audio_path = record_audio()\n",
        "    mfcc = extract_mfcc(audio_path)\n",
        "    mfcc = np.expand_dims(mfcc, axis=(0, -1))\n",
        "\n",
        "    prediction = model.predict(mfcc)\n",
        "    emotion = emotion_labels[np.argmax(prediction)]\n",
        "\n",
        "    print(f\"ðŸŽ¯ Predicted Emotion: {emotion.upper()}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    predict_emotion()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "Ti6HbF_WHrYk",
        "outputId": "6fc99183-f1b3-4416-b886-cf00e775333c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyaudio'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-877856540.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyaudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyaudio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XR9ZXwnpHwp3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}